{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning for Image Classification\n",
    "\n",
    "This notebook is a modified version of the code for the omniglot problem for an image classification problem, which involves classifying two different animals. I used a handpicked dataset of alligator and leopard pictures (20 images per class) for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Model.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"C:\\Users\\Dev1\\Documents\\GitHub\\How-to-Learn-from-Little-Data\\MANN\\Model.py\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    def step((M_tm1, c_tm1, h_tm1, r_tm1, wr_tm1, wu_tm1), (x_t)):\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Importing the dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "\n",
    "from MANN.Model import memory_augmented_neural_network\n",
    "from MANN.Utils.Generator import AnimalDataGenerator\n",
    "from MANN.Utils.Metrics import accuracy_instance\n",
    "from MANN.Utils.tf_utils import update_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memory_model():\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, (10, 50, 200)) #(batch_size, time, input_dimensions)\n",
    "    targets = tf.placeholder(tf.float32, (10, 50)) #(batch_size, time)  for the labels \n",
    "    \n",
    "    ## Modified global variables from the omniglot problem \n",
    "    nb_reads = 4\n",
    "    controller_size = 200\n",
    "    memory_shape = (128,40)\n",
    "    nb_class = 2\n",
    "    input_size = 20*20\n",
    "    batch_size = 10\n",
    "    nb_samples_per_class = 20\n",
    "    \n",
    "    # Helper class for loading data\n",
    "    generator = AnimalDataGenerator(data_folder='./data', batch_size=batch_size, \n",
    "                nb_samples=nb_class, nb_samples_per_class=nb_samples_per_class, max_rotation=0., max_shift=0., max_iter=None)\n",
    "    output_var, output_var_flatten, params = memory_augmented_neural_network(inputs, targets, batch_size=batch_size, \n",
    "        nb_class=nb_class, memory_shape=memory_shape, controller_size=controller_size, input_size=input_size, nb_reads=nb_reads)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"Weights\", reuse=True):\n",
    "        W_key = tf.get_variable('W_key', shape=(nb_reads, controller_size, memory_shape[1]))\n",
    "        b_key = tf.get_variable('b_key', shape=(nb_reads, memory_shape[1]))\n",
    "        W_add = tf.get_variable('W_add', shape=(nb_reads, controller_size, memory_shape[1]))\n",
    "        b_add = tf.get_variable('b_add', shape=(nb_reads, memory_shape[1]))\n",
    "        W_sigma = tf.get_variable('W_sigma', shape=(nb_reads, controller_size, 1))\n",
    "        b_sigma = tf.get_variable('b_sigma', shape=(nb_reads, 1))\n",
    "        W_xh = tf.get_variable('W_xh', shape=(input_size + nb_class, 4 * controller_size))\n",
    "        b_h = tf.get_variable('b_xh', shape=(4 * controller_size))\n",
    "        W_o = tf.get_variable('W_o', shape=(controller_size + nb_reads * memory_shape[1], nb_class))\n",
    "        b_o = tf.get_variable('b_o', shape=(nb_class))\n",
    "        W_rh = tf.get_variable('W_rh', shape=(nb_reads * memory_shape[1], 4 * controller_size))\n",
    "        W_hh = tf.get_variable('W_hh', shape=(controller_size, 4 * controller_size))\n",
    "        gamma = tf.get_variable('gamma', shape=[1], initializer=tf.constant_initializer(0.95))\n",
    "\n",
    "    params = [W_key, b_key, W_add, b_add, W_sigma, b_sigma, W_xh, W_rh, W_hh, b_h, W_o, b_o]\n",
    "    \n",
    "    \n",
    "     #output_var = tf.cast(output_var, tf.int32)\n",
    "    target_one_hot = tf.one_hot(targets, depth=generator.nb_samples)\n",
    "    print 'Output, Target shapes: ',output_var.get_shape().as_list(), target_one_hot.get_shape().as_list()\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_var, target_one_hot), name=\"cost\")\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    train_step = opt.minimize(cost, var_list=params)\n",
    "\n",
    "    #train_step = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "    accuracies = accuracy_instance(tf.argmax(output_var, axis=2), targets, batch_size=generator.batch_size)\n",
    "    sum_out = tf.reduce_sum(tf.reshape(tf.one_hot(tf.argmax(output_var, axis=2), depth=generator.nb_samples), (-1, generator.nb_samples)), axis=0)\n",
    "\n",
    "    print 'Done'\n",
    "\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    for i in range(generator.nb_samples_per_class):\n",
    "    \ttf.summary.scalar('accuracy-'+str(i), accuracies[i])\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    #writer = tf.summary.FileWriter('/tmp/tensorflow', graph=tf.get_default_graph())\n",
    "    train_writer = tf.summary.FileWriter('/tmp/tensorflow/', sess.graph)\n",
    "\n",
    "    t0 = time.time()\n",
    "    all_scores, scores, accs = [],[],np.zeros(generator.nb_samples_per_class)\n",
    "\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print 'Training the model'\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        for i, (batch_input, batch_output) in generator:\n",
    "            feed_dict = {\n",
    "                inputs: batch_input,\n",
    "                targets: batch_output\n",
    "            }\n",
    "            #print batch_input.shape, batch_output.shape\n",
    "            train_step.run(feed_dict)\n",
    "            score = cost.eval(feed_dict)\n",
    "            acc = accuracies.eval(feed_dict)\n",
    "            temp = sum_out.eval(feed_dict)\n",
    "            summary = merged.eval(feed_dict)\n",
    "            train_writer.add_summary(summary, i)\n",
    "            print i, ' ',temp\n",
    "            all_scores.append(score)\n",
    "            scores.append(score)\n",
    "            accs += acc\n",
    "            if i>0 and not (i%100):\n",
    "                print(accs / 100.0)\n",
    "                print('Episode %05d: %.6f' % (i, np.mean(score)))\n",
    "                scores, accs = [], np.zeros(generator.nb_samples_per_class)\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print time.time() - t0\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    memory_model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
